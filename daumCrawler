# -*- coding: utf-8 -*-
import scrapy
from scrapy.crawler import CrawlerProcess


class MyCrawler(scrapy.Spider):
    name = 'DAUM_CRAWLER'
    start_urls = ['http://www.daum.net']

    def parse(self, response):
        title1 = response.css("#realTimeSearchWord > li > div > div > span > a > strong::text").extract()
        title = response.css("#realTimeSearchWord > li > div > div > span > a::text").extract()
        url = response.css("#realTimeSearchWord > li > div > div > span > a::attr(href)").extract()

        realTitle = ['']*10
        realTitle[0] = title1[0]
        realUrl = ['']*10
        realUrl[0] = url[0]
        realTitle[9] = title[20][1:-1]
        realUrl[9] = url[19]
        rank = 0
        for i in range(4,len(response.css("#realTimeSearchWord > li > div > div > span > a").extract())):
            if i%2==0:
                realTitle[i/2-1] = title[i][1:-1]

        for i in range(4, len(response.css("#realTimeSearchWord > li > div > div > span > a").extract())):
            if i % 2 == 0:
                realUrl[i/2-1] = url[i]



        results = zip(realTitle, realUrl)
        for title, url in results:
            rank = rank + 1
            print ("%dìœ„ :" %rank),
            print title, url


process = CrawlerProcess({
    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'
})

process.crawl(MyCrawler)
process.start()
